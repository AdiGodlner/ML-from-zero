{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0dea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "CATEGORIES = 10\n",
    "TEST_SIZE = 10000\n",
    "\n",
    "# hayper parameters\n",
    "BATCH_SIZE = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1eff9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adi/anaconda3/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Fetch MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Assigning the features to the 'data' variable and labels to the 'labels' variable.\n",
    "data, labels = mnist['data'], mnist['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b0a6568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "### preprocessing \n",
    "\n",
    "def preprocess(X, y):\n",
    "    \n",
    "    # normalize values to be between 0 and 1\n",
    "    X_normalized = X / 255\n",
    "    # insert column of ones as the first column of the data for bias calc down the line \n",
    "    size = X_normalized.shape[0]\n",
    "    X_tilda = np.insert(X_normalized, 0, np.ones(size), axis=1)\n",
    "\n",
    "    # one hot encode y values \n",
    "    # values = np.arange(CATEGORIES)\n",
    "    # one_hot_encoded = np.eye(CATEGORIES, dtype=int)[values]  # TODO what does lines do \n",
    "\n",
    "    # # create a dict to map integers 0-9 to one hot encoded arrays\n",
    "    # mapping = {value: one_hot_encoded[index] for index, value in enumerate(values)}\n",
    "    # y_one_hot = y.astype(int).map(mapping)\n",
    "    y = y.astype(int).values\n",
    "\n",
    "    # split the dataset to a test set and a training set \n",
    "    return train_test_split(X_tilda, y, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "# preprocess the data using the preprocess function\n",
    "data_train, data_test, labels_train, labels_test = preprocess(data, labels)\n",
    "\n",
    "print(\"done\")\n",
    "# Printing \"done\" to indicate the completion of the preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5355b28b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m binary_classifires \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     binary_classifier \u001b[38;5;241m=\u001b[39m BinaryClassifier(data_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], labels_train[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#     binary_classifier.train(data_train, labels_train)\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     binary_classifires\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,binary_classifier)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "# create a binary classifier class\n",
    "class BinaryClassifier:\n",
    "    \n",
    "    def __init__(self, features, class_label):\n",
    "        self.max_iterations = 100\n",
    "        self.weights = np.random.randn(1, features)\n",
    "        self.pocket_weights = self.weights\n",
    "        # the evaluation is done using in sample error whos values are between 0 and 1 \n",
    "        # so an inital pocket_weights_evaluation > 1 garuntees a TODO  \n",
    "        self.pocket_weights_evaluation = 2 \n",
    "        self.label = class_label\n",
    "        # TODO histogram of evaluations ?\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        # TODO batch data\n",
    "        y_train = self.preprocess(y_train) \n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # TODO if self.best_weights_success_rate not improved in x rounds break\n",
    "            # or if pocket has linearly seprable \n",
    "            self.train_single_round(X_train, y_train)\n",
    "            curr_eval = self.evaluate(X_train,y_train)\n",
    "            if curr_eval < self.pocket_weights_evaluation:\n",
    "                print(f\"round {i} | evaluation {curr_eval}\")\n",
    "                self.pocket_weights = self.weights\n",
    "                self.pocket_weights_evaluation = curr_eval\n",
    "                \n",
    "                if self.pocket_weights_evaluation == 0:\n",
    "                    # if we linearly seprated the data then there is no better clasifier \n",
    "                    # so we do not need to keep training to find one \n",
    "                    # so we exit the function \n",
    "                    return \n",
    "\n",
    "    def train_single_round(self, X_train, y_train):\n",
    "        \n",
    "        #loop over training data\n",
    "        for Xt, yt in zip(X_train, y_train):\n",
    "            \n",
    "            prediction = np.sign(np.dot(self.weights, Xt))\n",
    "            # run prediction until we get to a missclsified exapmle\n",
    "            if yt * prediction  < 0:\n",
    "                # update the wweights and exit the function \n",
    "                self.weights = self.weights + (yt * Xt)\n",
    "                return \n",
    "    \n",
    "    def evaluate(self, X_train, y_train) :\n",
    "        #  calculate all predictions for the training data \n",
    "        predictions =  np.sign(np.dot(self.pocket_weights, X_train.T)) \n",
    "        # count wroung predictions\n",
    "        wroung_predictions_count = np.count_nonzero(predictions != y_train)\n",
    "#         print(f\"wroung_predictions_count : {wroung_predictions_count}\")\n",
    "        # get the number of training examples \n",
    "        size = X_train.shape[0]\n",
    "        return wroung_predictions_count / size\n",
    "        \n",
    "    def preprocess(self, labels):\n",
    "        return np.where(labels == self.label, 1, -1)\n",
    "                \n",
    "    def predict(self, datom):\n",
    "        return np.sign(np.dot(self.pocket_weights, datom))\n",
    "    \n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f024a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train all binariy classifires 0 to 9 and store them in a list \n",
    "\n",
    "binary_classifires = []\n",
    "for i in range(10):\n",
    "    binary_classifier = BinaryClassifier(data_train.shape[1], labels_train[0])\n",
    "#     binary_classifier.train(data_train, labels_train)\n",
    "    binary_classifires.insert(0,binary_classifier)\n",
    "# print(\"done \")\n",
    "# y_train[:3]\n",
    "# np.argmax(y_train[:3])\n",
    "print(len(binary_classifires))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3147cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1264920056.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"from matplotlib import pyplot\\n\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# plot testing \n",
    "from matplotlib import pyplot\n",
    "\n",
    "for i in range(9):  \n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(np.reshape(X_train[i], (28, 28)), cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002e067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
